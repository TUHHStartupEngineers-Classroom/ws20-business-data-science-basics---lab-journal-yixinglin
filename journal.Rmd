---
title: "Journal (reproducible report)"
author: "Yixing Lin"
date: "`r format(Sys.time(), '%d.%m.%Y')`"
output:
  html_document:
    toc: true
    toc_float: true
    collapsed: false
    number_sections: true
    toc_depth: 3
    #code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message=FALSE,warning=FALSE, cache=TRUE)
#knitr::opts_chunk$set(message=FALSE,warning=FALSE, cache=FALSE)
```


# Challenge: Bike Sales

## Analyze the sales by location (state) with a bar plot. 

```{r plot_bike_sales1, fig.width=10, fig.height=7}
library("tidyverse")
library(lubridate)
bike_orderlines_wrangled_tbl <- read.csv('./data/bike_orderlines.csv')
# 8.1 Analyze the sales by location (state) with a bar plot.
bike_orderlines_wrangled_loc_tbl   <- bike_orderlines_wrangled_tbl %>% separate(col    = location ,
         into   = c("city", "state"),
         sep    = ", ")

sales_by_loc_tbl <- bike_orderlines_wrangled_loc_tbl %>%
  select(state, total_price) %>%
  group_by(state) %>%
  summarize(sales = sum(total_price)) %>%
  mutate(sales_text = scales::dollar(sales, big.mark = ".",
                                     decimal.mark = ",",
                                     prefix = "",
                                     suffix = " €")) %>%
  arrange(desc(sales))

sales_by_loc_tbl %>% ggplot(aes(x = state, y = sales)) +

  # Geometries
  geom_col(fill = "#2DC6D6") + # Use geom_col for a bar plot
  geom_label(aes(label = sales_text)) + # Adding labels to the bars
  geom_smooth(method = "lm", se = FALSE) + # Adding a trendline

  # Formatting
  # scale_y_continuous(labels = scales::dollar) + # Change the y-axis.
  # Again, we have to adjust it for euro values
  scale_y_continuous(labels = scales::dollar_format(big.mark = ".",
                                                    decimal.mark = ",",
                                                    prefix = "",
                                                    suffix = " €")) +
  labs(
    title    = "Revenue by state",
    x = "State", # Override defaults for x and y
    y = "Revenue"
  ) + theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

The diagram shows that North Rhine-Westphalia has the highest revenue. 

## Analyze the sales by location and year (facet_wrap)

```{r plot_bike_sales2, fig.width=10, fig.height=7}
# 8.2 Analyze the sales by location and year (facet_wrap)
sales_by_loc_year_tbl <- bike_orderlines_wrangled_loc_tbl %>%
  mutate(year = year(order_date)) %>%
  select(state, year, total_price) %>%
  group_by(year, state) %>% 
  summarize(sales = sum(total_price)) %>%
  ungroup() %>%
  mutate(sales_text = scales::dollar(sales, big.mark = ".", 
                                     decimal.mark = ",", 
                                     prefix = "", 
                                     suffix = " €"))
  

sales_by_loc_year_tbl %>%
  ggplot(aes(x = year, y = sales, fill = state)) +
  facet_wrap(~ state) + 
  geom_col() + # Run up to here to get a stacked bar plot
  scale_y_continuous(labels = scales::dollar_format(big.mark = ".", 
                                                    decimal.mark = ",", 
                                                    prefix = "", 
                                                    suffix = " €")) +
labs(
  title = "Revenue by state and year",
  fill = "State" # Changes the legend name
)
rm(list = ls())
```



# Challenge: Data Acquistition

## Get some data via an API

Weather forecast of Hamburg. 
Provider: http://www.7timer.info/doc.php?lang=en

```{r table_weather_forecast}
library(httr)
library(kableExtra)
url = "http://www.7timer.info/bin/astro.php?lon=+10.000&lat=+53.6000&ac=0&unit=metric&output=json&tzshift=0"
resp <- GET(url)
library(jsonlite)
df_weather  <-  resp %>% 
  .$content %>% 
  rawToChar() %>% 
  fromJSON() %>%
  as.data.frame()

head(df_weather, 10) %>%
  kbl() %>%
 kable_paper("hover", full_width = F)
   

rm(list = ls())

```

## Scrape a competitor websites of canyon

Scrape competitor websites of canyon (https://www.rosebikes.de/), and create a small database.

```{r eval=F}
library(tidyverse) # Main Package - Loads dplyr, purrr, etc.
library(rvest)     # HTML Hacking & Web Scraping
library(xopen)     # Quickly opening URLs
library(jsonlite)  # converts JSON files to R objects
library(glue)      # concatenate strings
library(stringi)   # character string/text processing


# Get URL for each category.
get_bike_category_data <- function (bike_family_url) {
  
  html_bike_family  <- read_html(bike_family_url)
  
  bike_category_name_tbl <- html_bike_family %>%
    html_nodes('.catalog-category-bikes__list-item .catalog-category-bikes__title-text') %>%
    html_text() %>%
    stringr::str_replace_all("[\n\r]", "") %>%
    enframe(name = "position", value = "category") %>%
    as_tibble()
  
  bike_category_url_tbl <- html_bike_family %>%
    html_nodes('.catalog-category-bikes__list-item .catalog-category-bikes__button') %>%
    html_attr("href") %>%
    enframe(name = "position", value = "url") %>% 
    mutate(url = glue(paste(url_home, "{url}", sep=""))) %>%
    distinct(url) %>%
    as_tibble()
  
  bike_category_tbl <- tibble(bike_category_name_tbl,  bike_category_url_tbl)
}

# Get URL for each individual bike of each product category.
get_bike_model_data <- function(bike_category_url) {
  
  html_bike_category  <- read_html(bike_category_url)
  bike_model_name_tbl <- html_bike_category %>%
    html_nodes('.catalog-category-model__top .catalog-category-model__title') %>%
    html_text() %>%
    stringr::str_replace_all("[\n\r]", "")  %>%
    enframe(name = "position", value = "model.name") %>%
    as_tibble()
  
  bike_model_price_tbl <- html_bike_category %>%
    html_nodes('.product-tile-price__wrapper .catalog-category-model__price-current-value') %>%
    html_text() %>%
    stringr::str_replace_all("[\n\r€]", "")  %>%
    readr::parse_number(locale = readr::locale(decimal_mark = ",")) %>%
    enframe(name = "position", value = "price.in.euro") %>%
    as_tibble()
  bike_model_tbl <- tibble(bike_model_name_tbl,  bike_model_price_tbl[, "price.in.euro"])
}

# Read the homepage
url_home = "https://www.rosebikes.de"
url_bike = paste(url_home, "/fahrräder", sep="")
xopen(url_bike)
html_home  <- read_html(url_bike)
# Step 1.1  Get bike family urls
## extract bike family
bike_family_tbl <- html_home  %>%
    html_nodes(css = ".catalog-navigation__list .catalog-navigation__link")  %>% 
    html_attr("href") %>% discard(.p = ~stringr::str_detect(.x,"sale|zoovu|OUTLET|kinder|e-bike"))  %>% 
    stringr::str_extract("(?<=\\/fahrräder\\/)[a-zA-z0-9äöüßÄÖÜ-]+" )  %>%
    enframe(name = "position", value = "family") %>%
    mutate(url = glue(paste(url_home, "/fahrräder/{family}", sep=""))) 


bike_data_tbl <- tibble()
for (i in seq_along(bike_family_tbl$url)) {
  print (i)
  print (bike_family_tbl$url[i])
  bike_family_url <- bike_family_tbl$url[i]
  bike_family <- bike_family_tbl$family[i]
  bike_category_tbl <- get_bike_category_data(bike_family_url)
  Sys.sleep(5)
  
  for (j in seq_along(bike_category_tbl$url)) {
    bike_category_url <- bike_category_tbl$url[j]
    bike_category <- bike_category_tbl$category[j]
    print (j)
    print (bike_category)
    bike_model_tbl <- get_bike_model_data(bike_category_url)
    Sys.sleep(5)
    bike_model_tbl$category <- bike_category
    bike_model_tbl$family <- bike_family
    
    bike_data_tbl <- bind_rows(bike_data_tbl, bike_model_tbl)
  
  }
}
saveRDS(bike_data_tbl, "data/bike_data_tbl.rds")
```

```{r}
bike_data_tbl <-  readRDS("data/bike_data_tbl.rds") %>%
  select("model.name" , "price.in.euro", "category", "family")

head(bike_data_tbl, 10) %>%
kbl() %>%
kable_paper("hover", full_width = F)
```

# Challenge: Patent Analysis

Load the data from files, i.e. "patent.tsv", "assignee.tsv" and "patent_assignee.tsv"

```{r load_data, eval=F}
                      
library(tidyverse)
library(vroom)
library(data.table)
library(lubridate)
# "patent.tsv", "patent.tsv", "assignee.tsv", "patent_assignee.tsv", and "uspc.tsv"
workspace = "D://workspace//R//dsb//DS_101//DS_101//"
# workspace = "./data/"
col_types <- list(
  id = col_character(),
  type = col_character(),
  number = col_character(),
  country = col_character(),
  date = col_date("%Y-%m-%d"),
  abstract = col_character(),
  title = col_character(),
  kind = col_character(),
  num_claims = col_double(),
  filename = col_character(),
  withdrawn = col_double()
)

patent_tbl <- vroom(
  file       = paste(workspace, "patent.tsv", sep = ""), 
  delim      = "\t", 
  col_names  = names(col_types),
  col_types  = col_types,
  na         = c("", "NA", "NULL"),
  skip = 1
)
setDT(patent_tbl)

col_types <- list(
  id = col_character(),
  type = col_double(),
  name_first = col_character(),
  name_last = col_character(),
  organization = col_character()
)

assignee_tbl <- vroom(
  file       = paste(workspace, "assignee.tsv", sep = ""), 
  delim      = "\t", 
  col_names  = names(col_types),
  col_types  = col_types,
  na         = c("", "NA", "NULL"),
  skip = 1
)

setDT(assignee_tbl)

col_types <- list(
  patent_id = col_character(),
  assignee_id = col_character(),
  location_id = col_character()
)

pat_asgn_tbl <- vroom(
  file       = paste(workspace, "patent_assignee.tsv",  sep = ""), 
  delim      = "\t", 
  col_names  = names(col_types),
  col_types  = col_types,
  na         = c("", "NA", "NULL"),
  skip = 1
)

setDT(pat_asgn_tbl)

col_types <- list(
  uuid = col_character(),
  patent_id = col_character(),
  mainclass_id = col_character(),
  subclass_id = col_character(),
  sequence = col_double()
)

uspc_tbl <- vroom(
  file       = paste(workspace,  "uspc.tsv",  sep = ""),
  delim      = "\t", 
  col_names  = names(col_types),
  col_types  = col_types,
  na         = c("", "NA", "NULL"),
  skip = 1
)

setDT(uspc_tbl)

```

## Patent Dominance

10 US companies with the most assigned/granted patents.

```{r eval=F}
# Combine tables by left Join
combined_tbl <- assignee_tbl[type==2, c(1,2,5)]  %>% setnames("id", "assignee_id") # us company
combined_tbl <- merge(pat_asgn_tbl, combined_tbl, by="assignee_id", all.x = T) 
combined_tbl <- merge(patent_tbl, combined_tbl, by.y = "patent_id", by.x = "number", all.x = T)
combined_tbl <- combined_tbl[, c("id","date","title","assignee_id","organization")][, year := year(date)] 
num_patents_comp <- combined_tbl[!is.na(organization), 
                                 .N, 
                                 by = organization][order(N,  decreasing = TRUE)][1:10]
setnames(num_patents_comp, "N", "number.of.patents")
write_rds (num_patents_comp, "data/num_patents_comp.rds")

```

```{r}
library(readr)
library(data.table)
library(kableExtra)
num_patents_comp <- read_rds("data/num_patents_comp.rds")

num_patents_comp[1:10, ] %>% 
  kbl(caption = "10 US companies with the most assigned/granted patents") %>%
  kable_paper("hover", full_width = F)
```

## Recent patent acitivity

The top 10 US companies with the most new granted patents for 2019.


```{r eval=F}
num_patents_comp2019 <- combined_tbl[year == 2019, ][!is.na(organization), 
                                                     .N, by = organization][order(N,  decreasing = TRUE)][1:10]
setnames(num_patents_comp2019, "N", "number.of.patents")
write_rds (num_patents_comp2019, "data/num_patents_comp2019.rds")

```

```{r}
num_patents_comp2019 <- read_rds("data/num_patents_comp2019.rds")
num_patents_comp2019[1:10, ] %>% 
  kbl(caption = "The top 10 US companies with the most new granted patents for 2019.") %>%
  kable_paper("hover", full_width = F)
```


## Innovation in Tech

Goal: list top 5 USPTO tech main classes for the top 10 companies (worldwide) with the most patents. This task will be seperated into 2 steps.

###  The top 10 companies (worldwide) with the most patents.

```{r eval=F}
# Combine the tables
combined_tbl <- assignee_tbl[,c(1, 2, 5)]  %>% setnames("id", "assignee_id") # us company
combined_tbl <- merge(pat_asgn_tbl, combined_tbl, by="assignee_id", all.x = T) 
combined_tbl <- merge(patent_tbl, combined_tbl[, type:=NULL], by.y = "patent_id", by.x = "number", all.x = T)
combined_tbl <- combined_tbl[, c("id", "date", "title", "assignee_id", "organization")]
# Find the top 10 companies (worldwide) with the most patents
num_patents_comp= combined_tbl[!is.na(organization), 
                               .N, 
                               by = organization][order(N,  decreasing = TRUE)][1:10]
setnames(num_patents_comp, "N", "number.of.patents")
write_rds (num_patents_comp, "data/num_patents_comp_world.rds")
```

```{r}
num_patents_comp = read_rds("data/num_patents_comp_world.rds")
num_patents_comp[1:10, ] %>% 
  kbl(caption = "The top 10 companies (worldwide) with the most patents") %>%
  kable_paper("hover", full_width = F)

```


### List top 5 USPTO for the 10 contries

```{r eval=F}
combined_tbl <- merge(combined_tbl, num_patents_comp[, flag := T], by = "organization")
combined_tbl <- combined_tbl[flag == T, c("organization","date", "id", "assignee_id")] # only contain 10 countries
combined_tbl <- merge(uspc_tbl, combined_tbl, by.x = "patent_id", by.y = "id",  all.x = T)
combined_tbl <- combined_tbl[!is.na(organization)]
main_classes_top5 <- combined_tbl[, .N, by="mainclass_id"][order(N,  decreasing = TRUE)][1:5]
setnames(main_classes_top5, "N", "num_classes")
write_rds (main_classes_top5, "data/main_classes_top5.rds")
```

```{r}
main_classes_top5 = read_rds("data/main_classes_top5.rds")
main_classes_top5[1:5, ] %>% 
  kbl(caption = " Top 5 USPTO for the 10 contries") %>%
  kable_paper("hover", full_width = F)
```

```{r}
# rm("combined_tbl", "assignee_tbl", "pat_asgn_tbl", "uspc_tbl", "patent_tbl")
rm(list = ls())
```

# Challenge: COVID-19

## Map the time course of the cumulative Covid-19 cases

```{r eval=F}
library(tidyverse)
library(ggplot2)
library(dplyr)
library(ggrepel)
covid_data_tbl <- read_csv("https://opendata.ecdc.europa.eu/covid19/casedistribution/csv")
covid_data_tbl$date <- as.Date(covid_data_tbl$dateRep, "%d/%m/%Y")

# Obtain date in 2020 in 5 countries
countries = c("United_States_of_America", "United_Kingdom", "Spain", "France", "Germany")
covid_data2020_tbl <- covid_data_tbl %>% select(countriesAndTerritories, cases, day, month, 
                                                year, date, continentExp) %>%
  filter(countriesAndTerritories %in% countries, year == 2020)

library(plyr)
# Data for Europe
data_eu2020 <- covid_data_tbl[covid_data_tbl$continentExp == "Europe" & covid_data_tbl$year == 2020,]
data_eu2020 <- aggregate(data_eu2020$cases, by=list(date = data_eu2020$date), FUN=sum) %>% setNames(c("date", "cases"))
data_eu2020$countriesAndTerritories <- "Europe"
covid_data2020_tbl <- rbind.fill(covid_data2020_tbl, data_eu2020)  
detach("package:plyr", unload=TRUE)

# Calculate cumulative cases
covid_data2020_tbl  <- 
      # sort by countries and date
      covid_data2020_tbl[order(covid_data2020_tbl$countriesAndTerritories, covid_data2020_tbl$date),]  %>%
      # calculate the cumulative cases grouped by countries
      group_by(countriesAndTerritories) %>%
      mutate(cum_cases = cumsum(cases))

cum_cases_ends <- covid_data2020_tbl %>% 
  group_by(countriesAndTerritories) %>% top_n(1, cum_cases) %>% 
        arrange(desc(cum_cases))  %>% 
  rename(max_cum_cases = cum_cases)

covid_data2020_tbl<- merge(x = covid_data2020_tbl, y = select(cum_cases_ends, countriesAndTerritories, max_cum_cases)
      , by = "countriesAndTerritories", all.x = TRUE)

covid_data2020_tbl <- covid_data2020_tbl %>% mutate(cum_cases_million =cum_cases/10^6)  %>% arrange(desc(max_cum_cases)) 
  
write_rds (covid_data2020_tbl, "data/covid_data2020_tbl.rds")
write_rds (cum_cases_ends, "data/cum_cases_ends.rds")

```

```{r}
library(tidyverse)
library(ggplot2)
library(dplyr)
library(ggrepel)
covid_data2020_tbl = read_rds("data/covid_data2020_tbl.rds")
cum_cases_ends = read_rds("data/cum_cases_ends.rds")

covid_data2020_tbl %>% ggplot(aes(x=date, y=cum_cases_million, color = countriesAndTerritories)) +
  geom_line(size = 0.9, linetype = 1) + 
  scale_x_date(date_labels = "%B", expand=c(0,20), date_breaks = "months") + 
  scale_y_continuous(labels = scales::unit_format(unit = "M"))+
  xlab("Year 2020") + ylab("Cumulative Cases")  + 
  labs(title = "COVID-19 cofirmed cases worldwide", color = "Continent/Country",
       subtitle = "As of 11/02/2020, Europe had more cases than the USA ") + 
  
  scale_color_manual(values=c("green3", "blue",  "#48D1CC",  "#FFD700" , "#FF00FF" , "deeppink1")) + 
  ggrepel::geom_label_repel(data = cum_cases_ends[1:2,], show.legend= FALSE,
                            #min.segment.length = unit(0, 'lines'), nudge_y = 1,
            mapping=aes(x=date, y=max_cum_cases/10^6, 
              label=scales::comma(max_cum_cases), 
              ), 
              label.padding=0.3, 
              point.padding = 3, size = 5, nudge_x = -200) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45), 
        panel.border = element_blank(),
        legend.position="bottom")

```

## Visualize the distribution of the mortality rate

```{r eval=F}
library(tidyverse)
library(ggplot2)
library(dplyr)
library(ggrepel)
covid_data_tbl <- read_csv("https://opendata.ecdc.europa.eu/covid19/casedistribution/csv")
covid_data_tbl$date <- as.Date(covid_data_tbl$dateRep, "%d/%m/%Y")

covid_sum_by_countries <- covid_data_tbl %>%  group_by(countriesAndTerritories) %>% 
                summarise(deaths = sum(deaths), cases = sum(cases), popData2019 = max(popData2019)) 

covid_sum_by_countries <- covid_sum_by_countries %>% mutate(mortality.rate = deaths/popData2019)

write_rds (covid_sum_by_countries, "data/covid_sum_by_countries.rds")

```

```{r}
covid_sum_by_countries = read_rds("data/covid_sum_by_countries.rds")

world <- map_data("world")

covid_sum_by_countries <- covid_sum_by_countries %>% 
  mutate(across(countriesAndTerritories, str_replace_all, "_", " ")) %>%
  mutate(countriesAndTerritories = case_when(
    
    countriesAndTerritories == "United Kingdom" ~ "UK",
    countriesAndTerritories == "United States of America" ~ "USA",
    countriesAndTerritories == "Czechia" ~ "Czech Republic",
    TRUE ~ countriesAndTerritories
    
  ))

covid_sum_by_countries %>% ggplot(aes(map_id = countriesAndTerritories)) +
      labs(title = "Confirmed COVID-19 deaths relative to the size of the population",
          subtitle = "More than 1.2 Million confirmed COVID-19 deaths worldwide", 
          caption = format(Sys.time(), "Date: %d/%m/%Y")) +
      geom_map(dat=world, map = world, 
           aes(map_id=region), fill="#CCCCCC", color="white") +
      geom_map(aes(fill = mortality.rate), map = world, color = "white", size = 0.1) +
      expand_limits(x = world$long, y = world$lat) +
      scale_fill_gradient(low='red', high='#101010', 
                          name = "Mortality Rate", 
                          breaks = c(0, 3e-4, 6e-4, 9e-4, 12e-4, 15e-4), 
                          labels = c("0.000%", "0.030%", "0.060%", "0.090%", "0.120%", "0.150%"))+
      
      theme_bw() +
       theme(axis.title.x=element_blank(),
          axis.text.x=element_blank(),
          axis.ticks.x=element_blank(),
          axis.title.y=element_blank(),
          axis.text.y=element_blank(),
          axis.ticks.y=element_blank())


```



